{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ayiti Analytics capstone project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Background of different professional in Haiti from LinkedIn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scrapping LinkedIn to find link profile of different people in Haiti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import requests\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create values Selenium\n",
    "USERNAME = \"chosson.jeanfranco@gmail.com\" \n",
    "PASSWORD =  \"**********************\"\n",
    "linkedin = 'https://www.linkedin.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Selenium\n",
    "browser = webdriver.Chrome()\n",
    "browser.get(linkedin)\n",
    "time.sleep(3)\n",
    "# Identify email and password inputs and enter in information\n",
    "email = browser.find_element_by_name('session_key')\n",
    "password = browser.find_element_by_name('session_password')\n",
    "email.send_keys(USERNAME + Keys.RETURN)\n",
    "password.send_keys(PASSWORD + Keys.RETURN)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to build a data set of at least one thousand profiles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinkedIn puts restrictions on its page to prevent scrapping. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use the LinkedIn filter to search, if you search for people and filter by a specific country, you can get 100 pages with less than 11 people per page. it can be 10 or 5 people per page. \n",
    "In order to build my dataset, i will scrape linkedIn by searching for people and filter one by one for Haiti, some universities in Haiti, some companies in Haiti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to search page and scroll to the bottom of the page to load elements of the page\n",
    "search = \"https://www.linkedin.com/search/results/people/?facetGeoRegion=%5B%22ht%3A0%22%5D&origin=FACETED_SEARCH\"\n",
    "browser.get(search)\n",
    "time.sleep(3)\n",
    "browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "current_url = 'url_placeholder' # this is a placeholder for the URL check\n",
    "# Create empty dataframe\n",
    "df = pd.DataFrame(columns = ['name', 'title', 'location', 'profile'])\n",
    "\n",
    "debut_url = 'https://www.linkedin.com/search/results/people/?keywords=haiti&origin=SWITCH_SEARCH_VERTICAL&page='\n",
    "\n",
    "for i in range(100):\n",
    "    Final_url = debut_url + str(i)\n",
    "    browser.get(Final_url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    page = BeautifulSoup(browser.page_source, 'lxml')\n",
    "    page_names = page.find_all('span', class_ = 'actor-name')\n",
    "    page_titles = page.find_all('p', class_ = 'subline-level-1')\n",
    "    page_locations = page.find_all('p', class_ = 'subline-level-2')\n",
    "    page_profiles = page.find_all('a', class_ = 'search-result__result-link')\n",
    "    \n",
    "    # Put scraped data into a dataframe\n",
    "    names = list(map(lambda x: x.text, page_names))\n",
    "    titles = list(map(lambda x: x.text.replace('\\n', ''), page_titles))\n",
    "    locations = list(map(lambda x: x.text.replace('\\n', ''), page_locations))\n",
    "    profiles = list(map(lambda x: linkedin + x['href'], page_profiles))[::2]\n",
    "    temp = pd.DataFrame({'name':names, 'title':titles, 'location':locations, 'profile':profiles})\n",
    "    \n",
    "    # Filter out members who do not provide information\n",
    "    temp = temp[temp['name'] != 'LinkedIn Member']\n",
    "    if temp.shape[0]<10:\n",
    "        print(len(temp))\n",
    "    # Append new data to df\n",
    "    df = df.append(temp)\n",
    "    \n",
    "     # Stop appending if the number of retrieved records exceeds the limit\n",
    "    if df.shape[0] >= 1000:\n",
    "        break\n",
    "    time.sleep(3)\n",
    "        \n",
    "# Reset dataframe index\n",
    "df.reset_index()\n",
    "\n",
    "# Export results\n",
    "#df.to_csv(\"output_search.csv\", index = False)\n",
    "\n",
    "# Close Selenium\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create values Selenium\n",
    "USERNAME = \"chosson.jeanfranco@gmail.com\" \n",
    "PASSWORD =  \"**********************\"\n",
    "linkedin = 'https://www.linkedin.com'\n",
    "# Open Selenium\n",
    "browser = webdriver.Chrome()\n",
    "browser.get(linkedin)\n",
    "time.sleep(3)\n",
    "# Identify email and password inputs and enter in information\n",
    "email = browser.find_element_by_name('session_key')\n",
    "password = browser.find_element_by_name('session_password')\n",
    "email.send_keys(USERNAME + Keys.RETURN)\n",
    "password.send_keys(PASSWORD + Keys.RETURN)\n",
    "# Go to profile page and scroll to the bottom of the page to load elements of the page\n",
    "time.sleep(3)\n",
    "search = \"https://www.linkedin.com/search/results/people/?facetSchool=%5B%22162070%22%2C%225097000%22%5D&origin=FACETED_SEARCH\"\n",
    "browser.get(search)\n",
    "time.sleep(3)\n",
    "browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "current_url = 'url_placeholder' # this is a placeholder for the URL check\n",
    "# Create empty dataframe\n",
    "df = pd.DataFrame(columns = ['name', 'title', 'location', 'profile'])\n",
    "\n",
    "debut_url = 'https://www.linkedin.com/search/results/people/?facetSchool=%5B%22162070%22%2C%225097000%22%5D&origin=FACETED_SEARCH&page='\n",
    "\n",
    "for i in range(100):\n",
    "    Final_url = debut_url + str(i)\n",
    "    browser.get(Final_url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    page = BeautifulSoup(browser.page_source, 'lxml')\n",
    "    page_names = page.find_all('span', class_ = 'actor-name')\n",
    "    page_titles = page.find_all('p', class_ = 'subline-level-1')\n",
    "    page_locations = page.find_all('p', class_ = 'subline-level-2')\n",
    "    page_profiles = page.find_all('a', class_ = 'search-result__result-link')\n",
    "    \n",
    "    # Put scraped data into a dataframe\n",
    "    names = list(map(lambda x: x.text, page_names))\n",
    "    titles = list(map(lambda x: x.text.replace('\\n', ''), page_titles))\n",
    "    locations = list(map(lambda x: x.text.replace('\\n', ''), page_locations))\n",
    "    profiles = list(map(lambda x: linkedin + x['href'], page_profiles))[::2]\n",
    "    temp = pd.DataFrame({'name':names, 'title':titles, 'location':locations, 'profile':profiles})\n",
    "    \n",
    "    # Filter out members who do not provide information\n",
    "    temp = temp[temp['name'] != 'LinkedIn Member']\n",
    "    \n",
    "    # Append new data to df\n",
    "    df = df.append(temp)\n",
    "    \n",
    "     # Stop appending if the number of retrieved records exceeds the limit\n",
    "    if df.shape[0] >= 1000:\n",
    "        break\n",
    "    time.sleep(3)\n",
    "        \n",
    "# Reset dataframe index\n",
    "df.reset_index()\n",
    "\n",
    "# Export results\n",
    "df.to_csv(\"output_search1.csv\", index = False)\n",
    "\n",
    "# Close Selenium\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create values Selenium\n",
    "USERNAME = \"chosson.jeanfranco@gmail.com\" \n",
    "PASSWORD =  \"*************************\"\n",
    "linkedin = 'https://www.linkedin.com'\n",
    "# Open Selenium\n",
    "browser = webdriver.Chrome()\n",
    "browser.get(linkedin)\n",
    "time.sleep(3)\n",
    "# Identify email and password inputs and enter in information\n",
    "email = browser.find_element_by_name('session_key')\n",
    "password = browser.find_element_by_name('session_password')\n",
    "email.send_keys(USERNAME + Keys.RETURN)\n",
    "password.send_keys(PASSWORD + Keys.RETURN)\n",
    "# Go to profile page and scroll to the bottom of the page to load elements of the page\n",
    "time.sleep(3)\n",
    "\n",
    "search = \"https://www.linkedin.com/search/results/people/?facetPastCompany=%5B%22163457%22%2C%22767299%22%2C%225654%22%2C%221860%22%2C%221964533%22%5D&origin=FACETED_SEARCH\"\n",
    "browser.get(search)\n",
    "time.sleep(3)\n",
    "browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "current_url = 'url_placeholder' # this is a placeholder for the URL check\n",
    "# Create empty dataframe\n",
    "df = pd.DataFrame(columns = ['name', 'title', 'location', 'profile'])\n",
    "\n",
    "debut_url = 'https://www.linkedin.com/search/results/people/?facetPastCompany=%5B%22163457%22%2C%22767299%22%2C%225654%22%2C%221860%22%2C%221964533%22%5D&origin=FACETED_SEARCH&page='\n",
    "\n",
    "for i in range(100):\n",
    "    Final_url = debut_url + str(i)\n",
    "    browser.get(Final_url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    page = BeautifulSoup(browser.page_source, 'lxml')\n",
    "    page_names = page.find_all('span', class_ = 'actor-name')\n",
    "    page_titles = page.find_all('p', class_ = 'subline-level-1')\n",
    "    page_locations = page.find_all('p', class_ = 'subline-level-2')\n",
    "    page_profiles = page.find_all('a', class_ = 'search-result__result-link')\n",
    "    \n",
    "    # Put scraped data into a dataframe\n",
    "    names = list(map(lambda x: x.text, page_names))\n",
    "    titles = list(map(lambda x: x.text.replace('\\n', ''), page_titles))\n",
    "    locations = list(map(lambda x: x.text.replace('\\n', ''), page_locations))\n",
    "    profiles = list(map(lambda x: linkedin + x['href'], page_profiles))[::2]\n",
    "    temp = pd.DataFrame({'name':names, 'title':titles, 'location':locations, 'profile':profiles})\n",
    "    \n",
    "    # Filter out members who do not provide information\n",
    "    temp = temp[temp['name'] != 'LinkedIn Member']\n",
    "    \n",
    "    # Append new data to df\n",
    "    df = df.append(temp)\n",
    "    \n",
    "     # Stop appending if the number of retrieved records exceeds the limit\n",
    "    if df.shape[0] >= 1000:\n",
    "        break\n",
    "    time.sleep(3)\n",
    "        \n",
    "# Reset dataframe index\n",
    "df.reset_index()\n",
    "\n",
    "# Export results\n",
    "df.to_csv(\"output_search2.csv\", index = False)\n",
    "\n",
    "# Close Selenium\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output_search.csv')\n",
    "df1 = pd.read_csv('output_search1.csv')\n",
    "df2 = pd.read_csv('output_search2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concat the three datasets into result\n",
    "result = pd.concat([df, df1, df2], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Drop duplicates values on profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.drop_duplicates(subset =\"profile\", keep = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### make sure all the profile is from people in Haiti\n",
    "new_result = result[result['location'].str.contains('Haiti')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save dataset into a csv file.\n",
    "new_result.to_csv(\"result.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Haiti                                       1050\n",
       "        Port-au-Prince, Haiti                         57\n",
       "        Port-au-Prince Arrondissement, Haiti           8\n",
       "        Pétionville, Haiti                             4\n",
       "        Gros-Morne Arrondissement, Haiti               1\n",
       "        Jacmel Arrondissement, Haiti                   1\n",
       "        Trou-du-Nord Arrondissement, Haiti             1\n",
       "        Carrefour, Haiti                               1\n",
       "        Les Cayes Arrondissement, Haiti                1\n",
       "        Delmas, Haiti                                  1\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_result['location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1125, 4)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We have dataset of 1125 profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From those profiles, i want to get their background, education, (Bachelor degree's or master degree's), their study fields, Their experiences and their skills. \n",
    "i'm gonna do another scrape, to get those informations from the differents profiles i get before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.read_csv(\"result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty data frame\n",
    "Exp_df = pd.DataFrame(columns = ['profile', 'exp_title', 'exp_company', 'exp_dates'])\n",
    "Edu_df = pd.DataFrame(columns = ['profile', 'ed_name', 'ed_deg', 'ed_dates'])\n",
    "Ski_df = pd.DataFrame(columns = ['profile', 'skill'])\n",
    "\n",
    "tcounter = 0 \n",
    "#Create big loop\n",
    "#for link in r.loc[0:5,'profile']:\n",
    "for link in r.loc[:,'profile']:\n",
    "    if link == 'https://www.linkedin.com#':     #if it equal link then skip\n",
    "        continue \n",
    "    time.sleep(3)\n",
    "# This section is where you put in the profile link (loaded from the csv file) and browse to it\n",
    "    search = link\n",
    "    browser.get(search)\n",
    "    time.sleep(5)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    #browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    #raw = urlopen(link).read()\n",
    "    #page = BeautifulSoup(raw, \"html.parser\")\n",
    "    page = BeautifulSoup(browser.page_source, 'lxml')\n",
    "    \n",
    "    tcounter += 1\n",
    "    \n",
    "    if tcounter % 100 == 0:\n",
    "        time.sleep(1800)\n",
    "    \n",
    "#Experience Section  \n",
    "    titles = page.find_all('div', class_ = \"pv-entity__position-group-pager\")\n",
    "    companies = page.find_all('span', class_ = \"pv-entity__secondary-title\")\n",
    "    dates = page.find_all('h4', class_ = \"pv-entity__date-range\")\n",
    "\n",
    "    #Put scraped data into exp_df\n",
    "\n",
    "    arraylen1 = len(page.find_all('div', class_ = \"pv-entity__position-group-pager\"))\n",
    "\n",
    "    profile = link\n",
    "    exp_titles = list(map(lambda x: x.h3.text.strip(), titles))[0:arraylen1]\n",
    "    exp_companies = list(map(lambda x: x.text.strip(), companies))[0:arraylen1]\n",
    "    exp_dates = list(map(lambda x: x.text.strip().split('\\n')[-1], dates))[0:arraylen1]\n",
    "    \n",
    " #Education Section \n",
    "    institution = page.find_all('div', class_ = \"pv-entity__degree-info\")\n",
    "    degree = page.find_all('p', class_ = \"pv-entity__degree-name\")\n",
    "    dates = page.find_all('p', class_ = \"pv-entity__dates\")\n",
    " \n",
    "    #Put scraped data into edu_df\n",
    "    \n",
    "    arraylen2 = len(page.find_all('div', class_ = \"pv-entity__degree-info\"))\n",
    "\n",
    "    profile = link\n",
    "    ed_name = list(map(lambda x: x.text.strip().split('\\n')[-1], institution))[0:arraylen2]\n",
    "    ed_deg = list(map(lambda x: x.text.strip().split('\\n')[-1], degree))[0:arraylen2]\n",
    "    ed_dates = list(map(lambda x: x.text.strip().split('\\n')[-1], dates))[0:arraylen2]\n",
    "    if len(ed_dates) < arraylen2:\n",
    "        ed_dates = 'NA'\n",
    " #Skill Section \n",
    "    skill = page.find_all('span', class_ = \"pv-skill-category-entity__name-text\")\n",
    "    \n",
    "    #Put scraped data into a ski_df\n",
    "    \n",
    "    arraylen3 = len(page.find_all('span', class_ = \"pv-skill-category-entity__name-text\"))\n",
    "        \n",
    "    profile = link\n",
    "    skill = list(map(lambda x: x.text.strip(), skill))[0:arraylen3]\n",
    "    temp1 = pd.DataFrame({'profile':profile, 'exp_title':exp_titles, 'exp_company':exp_companies, 'exp_dates':exp_dates})\n",
    "    temp2 = pd.DataFrame({'profile':profile, 'ed_name':ed_name, 'ed_deg':ed_deg, 'ed_dates':ed_dates}) \n",
    "    temp3 = pd.DataFrame({'profile':profile, 'skill':skill})\n",
    "    Exp_df = Exp_df.append(temp1)\n",
    "    Edu_df = Edu_df.append(temp2)\n",
    "    Ski_df = Ski_df.append(temp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've been dealing with linkedIn's restrictions such as captcha, and I'm being asked to re-authenticate after an amount of request. \n",
    "so i need to repeat the process several times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty data frame\n",
    "Exp_df = pd.DataFrame(columns = ['profile', 'exp_title', 'exp_company', 'exp_dates'])\n",
    "Edu_df = pd.DataFrame(columns = ['profile', 'ed_name', 'ed_deg', 'ed_dates'])\n",
    "Ski_df = pd.DataFrame(columns = ['profile', 'skill'])\n",
    "\n",
    "tcounter = 0 \n",
    "#Create big loop\n",
    "#for link in r.loc[0:5,'profile']:\n",
    "for link in r.loc[:,'profile']:\n",
    "    if link == 'https://www.linkedin.com#':     #if it equal link then skip\n",
    "        continue \n",
    "    time.sleep(5)\n",
    "# This section is where you put in the profile link (loaded from the csv file) and browse to it\n",
    "    search = link\n",
    "    browser.get(search)\n",
    "    time.sleep(5)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    #browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    #raw = urlopen(link).read()\n",
    "    #page = BeautifulSoup(raw, \"html.parser\")\n",
    "    page = BeautifulSoup(browser.page_source, 'lxml')\n",
    "    \n",
    "    tcounter += 1\n",
    "    \n",
    "    if tcounter % 100 == 0:\n",
    "        time.sleep(1000)\n",
    "    \n",
    "    \n",
    "#Experience Section\n",
    "    tall = page.find_all('li', class_ = \"pv-entity__position-group-pager pv-profile-section__list-item ember-view\")\n",
    "    titles = []\n",
    "    companies = []\n",
    "    dates = []\n",
    "    \n",
    "    for li in tall:\n",
    "        _title = li.find_all('h3','t-16 t-black t-bold')\n",
    "        _companies = li.find_all('p','pv-entity__secondary-title t-14 t-black t-normal')\n",
    "        _dates = li.find_all('span', class_ = \"pv-entity__bullet-item-v2\")\n",
    "        if _title:\n",
    "            titles.append(_title[0].get_text().strip()) \n",
    "        else:\n",
    "             titles.append('')\n",
    "        if _companies:\n",
    "            companies.append(_companies[0].get_text().strip()) \n",
    "        else:\n",
    "             companies.append('')\n",
    "        if _dates:\n",
    "            dates.append(_dates[0].get_text().strip()) \n",
    "        else:\n",
    "             dates.append('')\n",
    "        print(companies)\n",
    "    #Put scraped data into exp_df\n",
    "\n",
    "\n",
    "    profile = link\n",
    "    exp_titles = titles\n",
    "    exp_companies =  companies\n",
    "    exp_dates = dates\n",
    "    \n",
    "    #Education Section\n",
    "    tall = page.find_all('li', class_ = \"pv-profile-section__list-item pv-education-entity pv-profile-section__card-item ember-view\")\n",
    "    study_name = [] \n",
    "    degree = []\n",
    "    dates = []\n",
    "    \n",
    "    for li in tall:\n",
    "        _std_name = li.find_all('p','pv-entity__secondary-title pv-entity__fos t-14 t-black t-normal')\n",
    "        _deg = li.find_all('p','pv-entity__secondary-title pv-entity__degree-name t-14 t-black t-normal')\n",
    "        _dates = li.find_all('p', class_ = \"pv-entity__dates t-14 t-black--light t-normal  time\")\n",
    "        if _std_name:\n",
    "            _study_name = _std_name[0].find_all('span', 'pv-entity__comma-item')\n",
    "            study_name.append(_study_name[0].get_text().strip()) \n",
    "        else:\n",
    "             study_name.append('')\n",
    "        if _deg:\n",
    "            _degree = _deg[0].find_all('span', 'pv-entity__comma-item')\n",
    "            degree.append(_degree[0].get_text().strip()) \n",
    "        else:\n",
    "             degree.append('')\n",
    "        if _dates:\n",
    "            dates.append(_dates[0].get_text().strip()) \n",
    "        else:\n",
    "             dates.append('')\n",
    "                \n",
    "    ed_name = study_name\n",
    "    ed_deg = degree\n",
    "    ed_dates = dates\n",
    "                \n",
    "    #Skill Section \n",
    "    tall = page.find_all('li', class_ = \"pv-skill-category-entity__top-skill pv-skill-category-entity pb3 pt4 pv-skill-endorsedSkill-entity relative ember-view\")\n",
    "    skill = []\n",
    "    \n",
    "    for li in tall:\n",
    "        _skill = li.find_all('span','pv-skill-category-entity__name-text t-16 t-black t-bold')\n",
    "        if _skill:\n",
    "            skill.append(_skill[0].get_text().strip()) \n",
    "        else:\n",
    "             skill.append('')\n",
    "\n",
    "        \n",
    "    temp1 = pd.DataFrame({'profile':profile, 'exp_title':exp_titles, 'exp_company':exp_companies, 'exp_dates':exp_dates})\n",
    "    temp2 = pd.DataFrame({'profile':profile, 'ed_name':ed_name, 'ed_deg':ed_deg, 'ed_dates':ed_dates}) \n",
    "    temp3 = pd.DataFrame({'profile':profile, 'skill':skill})\n",
    "    Exp_df = Exp_df.append(temp1)\n",
    "    Edu_df = Edu_df.append(temp2)\n",
    "    Ski_df = Ski_df.append(temp3)\n",
    "    print(link, 'completed')\n",
    "   \n",
    "\n",
    "Exp_df.to_csv('experience.csv')\n",
    "Edu_df.to_csv('education.csv')\n",
    "Ski_df.to_csv('skill.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I was able to scrape 559 profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newR = r.tail(566)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty data frame\n",
    "newExp_df = pd.DataFrame(columns = ['profile', 'exp_title', 'exp_company', 'exp_dates'])\n",
    "newEdu_df = pd.DataFrame(columns = ['profile', 'ed_name', 'ed_deg', 'ed_dates'])\n",
    "newSki_df = pd.DataFrame(columns = ['profile', 'skill'])\n",
    "\n",
    "tcounter = 0 \n",
    "#Create big loop\n",
    "#for link in r.loc[0:5,'profile']:\n",
    "for link in newR.loc[:,'profile']:\n",
    "    if link == 'https://www.linkedin.com#':     #if it equal link then skip\n",
    "        continue \n",
    "    time.sleep(5)\n",
    "# This section is where you put in the profile link (loaded from the csv file) and browse to it\n",
    "    search = link\n",
    "    browser.get(search)\n",
    "    time.sleep(5)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    #browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    #raw = urlopen(link).read()\n",
    "    #page = BeautifulSoup(raw, \"html.parser\")\n",
    "    page = BeautifulSoup(browser.page_source, 'lxml')\n",
    "    \n",
    "    tcounter += 1\n",
    "    \n",
    "    if tcounter % 100 == 0:\n",
    "        time.sleep(1000)\n",
    "    \n",
    "    \n",
    "#Experience Section\n",
    "    tall = page.find_all('li', class_ = \"pv-entity__position-group-pager pv-profile-section__list-item ember-view\")\n",
    "    titles = []\n",
    "    companies = []\n",
    "    dates = []\n",
    "    \n",
    "    for li in tall:\n",
    "        _title = li.find_all('h3','t-16 t-black t-bold')\n",
    "        _companies = li.find_all('p','pv-entity__secondary-title t-14 t-black t-normal')\n",
    "        _dates = li.find_all('span', class_ = \"pv-entity__bullet-item-v2\")\n",
    "        if _title:\n",
    "            titles.append(_title[0].get_text().strip()) \n",
    "        else:\n",
    "             titles.append('')\n",
    "        if _companies:\n",
    "            companies.append(_companies[0].get_text().strip()) \n",
    "        else:\n",
    "             companies.append('')\n",
    "        if _dates:\n",
    "            dates.append(_dates[0].get_text().strip()) \n",
    "        else:\n",
    "             dates.append('')\n",
    "        print(companies)\n",
    "    #Put scraped data into exp_df\n",
    "\n",
    "\n",
    "    profile = link\n",
    "    exp_titles = titles\n",
    "    exp_companies =  companies\n",
    "    exp_dates = dates\n",
    "    \n",
    "    #Education Section\n",
    "    tall = page.find_all('li', class_ = \"pv-profile-section__list-item pv-education-entity pv-profile-section__card-item ember-view\")\n",
    "    study_name = [] \n",
    "    degree = []\n",
    "    dates = []\n",
    "    \n",
    "    for li in tall:\n",
    "        _std_name = li.find_all('p','pv-entity__secondary-title pv-entity__fos t-14 t-black t-normal')\n",
    "        _deg = li.find_all('p','pv-entity__secondary-title pv-entity__degree-name t-14 t-black t-normal')\n",
    "        _dates = li.find_all('p', class_ = \"pv-entity__dates t-14 t-black--light t-normal  time\")\n",
    "        if _std_name:\n",
    "            _study_name = _std_name[0].find_all('span', 'pv-entity__comma-item')\n",
    "            study_name.append(_study_name[0].get_text().strip()) \n",
    "        else:\n",
    "             study_name.append('')\n",
    "        if _deg:\n",
    "            _degree = _deg[0].find_all('span', 'pv-entity__comma-item')\n",
    "            degree.append(_degree[0].get_text().strip()) \n",
    "        else:\n",
    "             degree.append('')\n",
    "        if _dates:\n",
    "            dates.append(_dates[0].get_text().strip()) \n",
    "        else:\n",
    "             dates.append('')\n",
    "                \n",
    "    ed_name = study_name\n",
    "    ed_deg = degree\n",
    "    ed_dates = dates\n",
    "                \n",
    "    #Skill Section \n",
    "    tall = page.find_all('li', class_ = \"pv-skill-category-entity__top-skill pv-skill-category-entity pb3 pt4 pv-skill-endorsedSkill-entity relative ember-view\")\n",
    "    skill = []\n",
    "    \n",
    "    for li in tall:\n",
    "        _skill = li.find_all('span','pv-skill-category-entity__name-text t-16 t-black t-bold')\n",
    "        if _skill:\n",
    "            skill.append(_skill[0].get_text().strip()) \n",
    "        else:\n",
    "             skill.append('')\n",
    "\n",
    "        \n",
    "    temp1 = pd.DataFrame({'profile':profile, 'exp_title':exp_titles, 'exp_company':exp_companies, 'exp_dates':exp_dates})\n",
    "    temp2 = pd.DataFrame({'profile':profile, 'ed_name':ed_name, 'ed_deg':ed_deg, 'ed_dates':ed_dates}) \n",
    "    temp3 = pd.DataFrame({'profile':profile, 'skill':skill})\n",
    "    newExp_df = newExp_df.append(temp1)\n",
    "    newEdu_df = newEdu_df.append(temp2)\n",
    "    newSki_df = newSki_df.append(temp3)\n",
    "    print(link, 'completed')\n",
    "   \n",
    "\n",
    "newExp_df.to_csv('experience2.csv')\n",
    "newEdu_df.to_csv('education2.csv')\n",
    "newSki_df.to_csv('skill2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for the next time i was able to get informations from 256 others profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_R = newR.tail(310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty data frame\n",
    "newExp_df = pd.DataFrame(columns = ['profile', 'exp_title', 'exp_company', 'exp_dates'])\n",
    "newEdu_df = pd.DataFrame(columns = ['profile', 'ed_name', 'ed_deg', 'ed_dates'])\n",
    "newSki_df = pd.DataFrame(columns = ['profile', 'skill'])\n",
    "\n",
    "tcounter = 0 \n",
    "#Create big loop\n",
    "#for link in r.loc[0:5,'profile']:\n",
    "for link in new_R.loc[:,'profile']:\n",
    "    if link == 'https://www.linkedin.com#':     #if it equal link then skip\n",
    "        continue \n",
    "    time.sleep(5)\n",
    "# This section is where you put in the profile link (loaded from the csv file) and browse to it\n",
    "    search = link\n",
    "    browser.get(search)\n",
    "    time.sleep(5)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    #browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    #raw = urlopen(link).read()\n",
    "    #page = BeautifulSoup(raw, \"html.parser\")\n",
    "    page = BeautifulSoup(browser.page_source, 'lxml')\n",
    "    \n",
    "    tcounter += 1\n",
    "    \n",
    "    if tcounter % 100 == 0:\n",
    "        time.sleep(900)\n",
    "    \n",
    "    \n",
    "#Experience Section\n",
    "    tall = page.find_all('li', class_ = \"pv-entity__position-group-pager pv-profile-section__list-item ember-view\")\n",
    "    titles = []\n",
    "    companies = []\n",
    "    dates = []\n",
    "    \n",
    "    for li in tall:\n",
    "        _title = li.find_all('h3','t-16 t-black t-bold')\n",
    "        _companies = li.find_all('p','pv-entity__secondary-title t-14 t-black t-normal')\n",
    "        _dates = li.find_all('span', class_ = \"pv-entity__bullet-item-v2\")\n",
    "        if _title:\n",
    "            titles.append(_title[0].get_text().strip()) \n",
    "        else:\n",
    "             titles.append('')\n",
    "        if _companies:\n",
    "            companies.append(_companies[0].get_text().strip()) \n",
    "        else:\n",
    "             companies.append('')\n",
    "        if _dates:\n",
    "            dates.append(_dates[0].get_text().strip()) \n",
    "        else:\n",
    "             dates.append('')\n",
    "        print(companies)\n",
    "    #Put scraped data into exp_df\n",
    "\n",
    "\n",
    "    profile = link\n",
    "    exp_titles = titles\n",
    "    exp_companies =  companies\n",
    "    exp_dates = dates\n",
    "    \n",
    "    #Education Section\n",
    "    tall = page.find_all('li', class_ = \"pv-profile-section__list-item pv-education-entity pv-profile-section__card-item ember-view\")\n",
    "    study_name = [] \n",
    "    degree = []\n",
    "    dates = []\n",
    "    \n",
    "    for li in tall:\n",
    "        _std_name = li.find_all('p','pv-entity__secondary-title pv-entity__fos t-14 t-black t-normal')\n",
    "        _deg = li.find_all('p','pv-entity__secondary-title pv-entity__degree-name t-14 t-black t-normal')\n",
    "        _dates = li.find_all('p', class_ = \"pv-entity__dates t-14 t-black--light t-normal  time\")\n",
    "        if _std_name:\n",
    "            _study_name = _std_name[0].find_all('span', 'pv-entity__comma-item')\n",
    "            study_name.append(_study_name[0].get_text().strip()) \n",
    "        else:\n",
    "             study_name.append('')\n",
    "        if _deg:\n",
    "            _degree = _deg[0].find_all('span', 'pv-entity__comma-item')\n",
    "            degree.append(_degree[0].get_text().strip()) \n",
    "        else:\n",
    "             degree.append('')\n",
    "        if _dates:\n",
    "            dates.append(_dates[0].get_text().strip()) \n",
    "        else:\n",
    "             dates.append('')\n",
    "                \n",
    "    ed_name = study_name\n",
    "    ed_deg = degree\n",
    "    ed_dates = dates\n",
    "                \n",
    "    #Skill Section \n",
    "    tall = page.find_all('li', class_ = \"pv-skill-category-entity__top-skill pv-skill-category-entity pb3 pt4 pv-skill-endorsedSkill-entity relative ember-view\")\n",
    "    skill = []\n",
    "    \n",
    "    for li in tall:\n",
    "        _skill = li.find_all('span','pv-skill-category-entity__name-text t-16 t-black t-bold')\n",
    "        if _skill:\n",
    "            skill.append(_skill[0].get_text().strip()) \n",
    "        else:\n",
    "             skill.append('')\n",
    "\n",
    "        \n",
    "    temp1 = pd.DataFrame({'profile':profile, 'exp_title':exp_titles, 'exp_company':exp_companies, 'exp_dates':exp_dates})\n",
    "    temp2 = pd.DataFrame({'profile':profile, 'ed_name':ed_name, 'ed_deg':ed_deg, 'ed_dates':ed_dates}) \n",
    "    temp3 = pd.DataFrame({'profile':profile, 'skill':skill})\n",
    "    newExp_df = newExp_df.append(temp1)\n",
    "    newEdu_df = newEdu_df.append(temp2)\n",
    "    newSki_df = newSki_df.append(temp3)\n",
    "    print(link, 'completed')\n",
    "    print(tcounter)\n",
    "    if tcounter  == 210:\n",
    "        break\n",
    "   \n",
    "\n",
    "newExp_df.to_csv('experience3.csv')\n",
    "newEdu_df.to_csv('education3.csv')\n",
    "newSki_df.to_csv('skill3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In that time i was able to get informations from 200 profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_R = new_R.tail(110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty data frame\n",
    "newExp_df = pd.DataFrame(columns = ['profile', 'exp_title', 'exp_company', 'exp_dates'])\n",
    "newEdu_df = pd.DataFrame(columns = ['profile', 'ed_name', 'ed_deg', 'ed_dates'])\n",
    "newSki_df = pd.DataFrame(columns = ['profile', 'skill'])\n",
    "\n",
    "tcounter = 0 \n",
    "#Create big loop\n",
    "#for link in r.loc[0:5,'profile']:\n",
    "for link in done_R.loc[:,'profile']:\n",
    "    if link == 'https://www.linkedin.com#':     #if it equal link then skip\n",
    "        continue \n",
    "    time.sleep(5)\n",
    "# This section is where you put in the profile link (loaded from the csv file) and browse to it\n",
    "    search = link\n",
    "    browser.get(search)\n",
    "    time.sleep(5)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "    #browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    #raw = urlopen(link).read()\n",
    "    #page = BeautifulSoup(raw, \"html.parser\")\n",
    "    page = BeautifulSoup(browser.page_source, 'lxml')\n",
    "    \n",
    "    tcounter += 1\n",
    "    \n",
    "    if tcounter % 100 == 0:\n",
    "        time.sleep(900)\n",
    "    \n",
    "    \n",
    "#Experience Section\n",
    "    tall = page.find_all('li', class_ = \"pv-entity__position-group-pager pv-profile-section__list-item ember-view\")\n",
    "    titles = []\n",
    "    companies = []\n",
    "    dates = []\n",
    "    \n",
    "    for li in tall:\n",
    "        _title = li.find_all('h3','t-16 t-black t-bold')\n",
    "        _companies = li.find_all('p','pv-entity__secondary-title t-14 t-black t-normal')\n",
    "        _dates = li.find_all('span', class_ = \"pv-entity__bullet-item-v2\")\n",
    "        if _title:\n",
    "            titles.append(_title[0].get_text().strip()) \n",
    "        else:\n",
    "             titles.append('')\n",
    "        if _companies:\n",
    "            companies.append(_companies[0].get_text().strip()) \n",
    "        else:\n",
    "             companies.append('')\n",
    "        if _dates:\n",
    "            dates.append(_dates[0].get_text().strip()) \n",
    "        else:\n",
    "             dates.append('')\n",
    "    #Put scraped data into exp_df\n",
    "\n",
    "\n",
    "    profile = link\n",
    "    exp_titles = titles\n",
    "    exp_companies =  companies\n",
    "    exp_dates = dates\n",
    "    \n",
    "    #Education Section\n",
    "    tall = page.find_all('li', class_ = \"pv-profile-section__list-item pv-education-entity pv-profile-section__card-item ember-view\")\n",
    "    study_name = [] \n",
    "    degree = []\n",
    "    dates = []\n",
    "    \n",
    "    for li in tall:\n",
    "        _std_name = li.find_all('p','pv-entity__secondary-title pv-entity__fos t-14 t-black t-normal')\n",
    "        _deg = li.find_all('p','pv-entity__secondary-title pv-entity__degree-name t-14 t-black t-normal')\n",
    "        _dates = li.find_all('p', class_ = \"pv-entity__dates t-14 t-black--light t-normal  time\")\n",
    "        if _std_name:\n",
    "            _study_name = _std_name[0].find_all('span', 'pv-entity__comma-item')\n",
    "            study_name.append(_study_name[0].get_text().strip()) \n",
    "        else:\n",
    "             study_name.append('')\n",
    "        if _deg:\n",
    "            _degree = _deg[0].find_all('span', 'pv-entity__comma-item')\n",
    "            degree.append(_degree[0].get_text().strip()) \n",
    "        else:\n",
    "             degree.append('')\n",
    "        if _dates:\n",
    "            dates.append(_dates[0].get_text().strip()) \n",
    "        else:\n",
    "             dates.append('')\n",
    "                \n",
    "    ed_name = study_name\n",
    "    ed_deg = degree\n",
    "    ed_dates = dates\n",
    "                \n",
    "    #Skill Section \n",
    "    tall = page.find_all('li', class_ = \"pv-skill-category-entity__top-skill pv-skill-category-entity pb3 pt4 pv-skill-endorsedSkill-entity relative ember-view\")\n",
    "    skill = []\n",
    "    \n",
    "    for li in tall:\n",
    "        _skill = li.find_all('span','pv-skill-category-entity__name-text t-16 t-black t-bold')\n",
    "        if _skill:\n",
    "            skill.append(_skill[0].get_text().strip()) \n",
    "        else:\n",
    "             skill.append('')\n",
    "\n",
    "        \n",
    "    temp1 = pd.DataFrame({'profile':profile, 'exp_title':exp_titles, 'exp_company':exp_companies, 'exp_dates':exp_dates})\n",
    "    temp2 = pd.DataFrame({'profile':profile, 'ed_name':ed_name, 'ed_deg':ed_deg, 'ed_dates':ed_dates}) \n",
    "    temp3 = pd.DataFrame({'profile':profile, 'skill':skill})\n",
    "    newExp_df = newExp_df.append(temp1)\n",
    "    newEdu_df = newEdu_df.append(temp2)\n",
    "    newSki_df = newSki_df.append(temp3)\n",
    "    print(link, 'completed')\n",
    "    print(tcounter)\n",
    "   \n",
    "\n",
    "newExp_df.to_csv('experience4.csv')\n",
    "newEdu_df.to_csv('education4.csv')\n",
    "newSki_df.to_csv('skill4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally i was able to get informations from all the profiles, of course those that had informations. Then i have 12 datasets, 4 for each, experience, education and skill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load these datasets in 3 datasets, for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_list = [\"experience.csv\", \"experience2.csv\", \"experience3.csv\", \"experience4.csv\"]\n",
    "\n",
    "list_of_dataframes = []\n",
    "for filename in csv_file_list:\n",
    "    list_of_dataframes.append(pd.read_csv(filename))\n",
    "\n",
    "experience_df = pd.concat(list_of_dataframes)\n",
    "\n",
    "csv_file_list = [\"education.csv\", \"education2.csv\", \"education3.csv\", \"education4.csv\"]\n",
    "\n",
    "list_of_dataframes = []\n",
    "for filename in csv_file_list:\n",
    "    list_of_dataframes.append(pd.read_csv(filename))\n",
    "\n",
    "education_df = pd.concat(list_of_dataframes)\n",
    "\n",
    "csv_file_list = [\"skill.csv\", \"skill2.csv\", \"skill3.csv\", \"skill4.csv\"]\n",
    "\n",
    "list_of_dataframes = []\n",
    "for filename in csv_file_list:\n",
    "    list_of_dataframes.append(pd.read_csv(filename))\n",
    "\n",
    "skill_df = pd.concat(list_of_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience_df.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "education_df.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "skill_df.drop('Unnamed: 0', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile</th>\n",
       "      <th>exp_title</th>\n",
       "      <th>exp_company</th>\n",
       "      <th>exp_dates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>https://www.linkedin.com/in/joseph-melior-3a4a...</td>\n",
       "      <td>CEO</td>\n",
       "      <td>CHOCOMAX-HAITI</td>\n",
       "      <td>6 yrs 7 mos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>https://www.linkedin.com/in/alexandre-michel-0...</td>\n",
       "      <td>Rédacteur</td>\n",
       "      <td>Journal l'Union, Haïti</td>\n",
       "      <td>2 yrs 9 mos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>https://www.linkedin.com/in/alexandre-michel-0...</td>\n",
       "      <td>Directeur exécutif</td>\n",
       "      <td>Tras-Haïti\\n        Self-employed</td>\n",
       "      <td>2 yrs 5 mos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>https://www.linkedin.com/in/biento-jacques-871...</td>\n",
       "      <td>Co-Founder</td>\n",
       "      <td>GBI Haiti</td>\n",
       "      <td>1 yr 9 mos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>https://www.linkedin.com/in/valery-fils-aime-6...</td>\n",
       "      <td>Co-fondateur et Resp. de partenariats</td>\n",
       "      <td>Haiti Climat\\n        Part-time</td>\n",
       "      <td>2 yrs 10 mos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>381</td>\n",
       "      <td>https://www.linkedin.com/in/markenley-belly-sa...</td>\n",
       "      <td>Procurement Officer</td>\n",
       "      <td>American Red Cross</td>\n",
       "      <td>9 yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>382</td>\n",
       "      <td>https://www.linkedin.com/in/markenley-belly-sa...</td>\n",
       "      <td>Procurement Officer</td>\n",
       "      <td>World Vision</td>\n",
       "      <td>10 mos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>383</td>\n",
       "      <td>https://www.linkedin.com/in/markenley-belly-sa...</td>\n",
       "      <td>Logistigien</td>\n",
       "      <td>Oxfam</td>\n",
       "      <td>2 yrs 5 mos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>https://www.linkedin.com/in/christelle-daniel-...</td>\n",
       "      <td>Marketing Communication Coordinator</td>\n",
       "      <td>Alternative Insurance Company</td>\n",
       "      <td>3 yrs 7 mos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>https://www.linkedin.com/in/christelle-daniel-...</td>\n",
       "      <td>Mobile Advertising Account Executive / AD Agen...</td>\n",
       "      <td>Digicel Group</td>\n",
       "      <td>2 yrs 9 mos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3210 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               profile  \\\n",
       "0    https://www.linkedin.com/in/joseph-melior-3a4a...   \n",
       "1    https://www.linkedin.com/in/alexandre-michel-0...   \n",
       "2    https://www.linkedin.com/in/alexandre-michel-0...   \n",
       "3    https://www.linkedin.com/in/biento-jacques-871...   \n",
       "4    https://www.linkedin.com/in/valery-fils-aime-6...   \n",
       "..                                                 ...   \n",
       "381  https://www.linkedin.com/in/markenley-belly-sa...   \n",
       "382  https://www.linkedin.com/in/markenley-belly-sa...   \n",
       "383  https://www.linkedin.com/in/markenley-belly-sa...   \n",
       "384  https://www.linkedin.com/in/christelle-daniel-...   \n",
       "385  https://www.linkedin.com/in/christelle-daniel-...   \n",
       "\n",
       "                                             exp_title  \\\n",
       "0                                                  CEO   \n",
       "1                                            Rédacteur   \n",
       "2                                   Directeur exécutif   \n",
       "3                                           Co-Founder   \n",
       "4                Co-fondateur et Resp. de partenariats   \n",
       "..                                                 ...   \n",
       "381                                Procurement Officer   \n",
       "382                                Procurement Officer   \n",
       "383                                        Logistigien   \n",
       "384                Marketing Communication Coordinator   \n",
       "385  Mobile Advertising Account Executive / AD Agen...   \n",
       "\n",
       "                           exp_company     exp_dates  \n",
       "0                       CHOCOMAX-HAITI   6 yrs 7 mos  \n",
       "1               Journal l'Union, Haïti   2 yrs 9 mos  \n",
       "2    Tras-Haïti\\n        Self-employed   2 yrs 5 mos  \n",
       "3                            GBI Haiti    1 yr 9 mos  \n",
       "4      Haiti Climat\\n        Part-time  2 yrs 10 mos  \n",
       "..                                 ...           ...  \n",
       "381                 American Red Cross         9 yrs  \n",
       "382                       World Vision        10 mos  \n",
       "383                              Oxfam   2 yrs 5 mos  \n",
       "384      Alternative Insurance Company   3 yrs 7 mos  \n",
       "385                      Digicel Group   2 yrs 9 mos  \n",
       "\n",
       "[3210 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile</th>\n",
       "      <th>ed_name</th>\n",
       "      <th>ed_deg</th>\n",
       "      <th>ed_dates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>https://www.linkedin.com/in/joseph-melior-3a4a...</td>\n",
       "      <td>Law</td>\n",
       "      <td>Bachelor's degree</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>https://www.linkedin.com/in/joseph-melior-3a4a...</td>\n",
       "      <td>Sociologie</td>\n",
       "      <td>Licentiate degree</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>https://www.linkedin.com/in/alexandre-michel-0...</td>\n",
       "      <td>Sciences de la Communication et des relations ...</td>\n",
       "      <td>Licence</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>https://www.linkedin.com/in/biento-jacques-871...</td>\n",
       "      <td>Photography</td>\n",
       "      <td>Diplome</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>https://www.linkedin.com/in/biento-jacques-871...</td>\n",
       "      <td>Broadcast Journalism</td>\n",
       "      <td>Bachelor's degree</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>https://www.linkedin.com/in/dexavier/</td>\n",
       "      <td>INFORMATIQUE</td>\n",
       "      <td>GESTION DE SYSTEME INFORMATISEE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>https://www.linkedin.com/in/dexavier/</td>\n",
       "      <td>Network and System Administration/Administrator</td>\n",
       "      <td>Computer Networking</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>https://www.linkedin.com/in/jhon-kerby-gerlin-...</td>\n",
       "      <td>Business Administration and Management, General</td>\n",
       "      <td>2nd</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>https://www.linkedin.com/in/ernest-s-sherman-2...</td>\n",
       "      <td>Logistics, Materials, and Supply Chain Management</td>\n",
       "      <td>Certificate in Humanitarian Logistics</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>https://www.linkedin.com/in/markenley-belly-sa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Licentiate degree</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2003 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               profile  \\\n",
       "0    https://www.linkedin.com/in/joseph-melior-3a4a...   \n",
       "1    https://www.linkedin.com/in/joseph-melior-3a4a...   \n",
       "2    https://www.linkedin.com/in/alexandre-michel-0...   \n",
       "3    https://www.linkedin.com/in/biento-jacques-871...   \n",
       "4    https://www.linkedin.com/in/biento-jacques-871...   \n",
       "..                                                 ...   \n",
       "199              https://www.linkedin.com/in/dexavier/   \n",
       "200              https://www.linkedin.com/in/dexavier/   \n",
       "201  https://www.linkedin.com/in/jhon-kerby-gerlin-...   \n",
       "202  https://www.linkedin.com/in/ernest-s-sherman-2...   \n",
       "203  https://www.linkedin.com/in/markenley-belly-sa...   \n",
       "\n",
       "                                               ed_name  \\\n",
       "0                                                  Law   \n",
       "1                                           Sociologie   \n",
       "2    Sciences de la Communication et des relations ...   \n",
       "3                                          Photography   \n",
       "4                                 Broadcast Journalism   \n",
       "..                                                 ...   \n",
       "199                                       INFORMATIQUE   \n",
       "200    Network and System Administration/Administrator   \n",
       "201    Business Administration and Management, General   \n",
       "202  Logistics, Materials, and Supply Chain Management   \n",
       "203                                                NaN   \n",
       "\n",
       "                                    ed_deg  ed_dates  \n",
       "0                        Bachelor's degree       NaN  \n",
       "1                        Licentiate degree       NaN  \n",
       "2                                  Licence       NaN  \n",
       "3                                  Diplome       NaN  \n",
       "4                        Bachelor's degree       NaN  \n",
       "..                                     ...       ...  \n",
       "199        GESTION DE SYSTEME INFORMATISEE       NaN  \n",
       "200                    Computer Networking       NaN  \n",
       "201                                    2nd       NaN  \n",
       "202  Certificate in Humanitarian Logistics       NaN  \n",
       "203                      Licentiate degree       NaN  \n",
       "\n",
       "[2003 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "education_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile</th>\n",
       "      <th>skill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>https://www.linkedin.com/in/joseph-melior-3a4a...</td>\n",
       "      <td>Strategic Planning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>https://www.linkedin.com/in/joseph-melior-3a4a...</td>\n",
       "      <td>Public Speaking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>https://www.linkedin.com/in/joseph-melior-3a4a...</td>\n",
       "      <td>Business Strategy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>https://www.linkedin.com/in/biento-jacques-871...</td>\n",
       "      <td>Graphic Design</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>https://www.linkedin.com/in/biento-jacques-871...</td>\n",
       "      <td>Video Production</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>https://www.linkedin.com/in/markenley-belly-sa...</td>\n",
       "      <td>Government</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>https://www.linkedin.com/in/markenley-belly-sa...</td>\n",
       "      <td>Nonprofits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>https://www.linkedin.com/in/christelle-daniel-...</td>\n",
       "      <td>Management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>https://www.linkedin.com/in/christelle-daniel-...</td>\n",
       "      <td>Direct Sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>https://www.linkedin.com/in/christelle-daniel-...</td>\n",
       "      <td>Salesforce.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2258 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               profile               skill\n",
       "0    https://www.linkedin.com/in/joseph-melior-3a4a...  Strategic Planning\n",
       "1    https://www.linkedin.com/in/joseph-melior-3a4a...     Public Speaking\n",
       "2    https://www.linkedin.com/in/joseph-melior-3a4a...   Business Strategy\n",
       "3    https://www.linkedin.com/in/biento-jacques-871...      Graphic Design\n",
       "4    https://www.linkedin.com/in/biento-jacques-871...    Video Production\n",
       "..                                                 ...                 ...\n",
       "246  https://www.linkedin.com/in/markenley-belly-sa...          Government\n",
       "247  https://www.linkedin.com/in/markenley-belly-sa...          Nonprofits\n",
       "248  https://www.linkedin.com/in/christelle-daniel-...          Management\n",
       "249  https://www.linkedin.com/in/christelle-daniel-...        Direct Sales\n",
       "250  https://www.linkedin.com/in/christelle-daniel-...      Salesforce.com\n",
       "\n",
       "[2258 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skill_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
